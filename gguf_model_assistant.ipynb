{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b929a468",
   "metadata": {},
   "source": [
    "# GGUF Model Assistant in Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424df76",
   "metadata": {},
   "source": [
    "\n",
    "In diesem Notebook wird ein GGUF-Modell von Hugging Face geladen, eine Chat-Funktion erstellt, das Modell trainiert und die Ergebnisse in Google Drive und Hugging Face gespeichert.\n",
    "Folge den Schritten, um das Projekt vollständig auszuführen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4526db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install transformers huggingface_hub accelerate datasets llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e6b6c",
   "metadata": {},
   "source": [
    "### Google Drive Mount (optional, falls Sie das Modell speichern möchten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5863aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0867febd",
   "metadata": {},
   "source": [
    "### Hugging Face API-Schlüssel eingeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9368329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()  # Du wirst nach deinem API-Schlüssel gefragt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a17ee",
   "metadata": {},
   "source": [
    "### GGUF-Modell laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96489cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Beispiel: GGUF-Modell laden\n",
    "model_path = \"TheBloke/Mistral-7B-GGUF\"  # Beispielmodell\n",
    "model = Llama(model_path=model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57078c33",
   "metadata": {},
   "source": [
    "### Chat-Funktion erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897909a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat(prompt, model):\n",
    "    response = model(prompt, max_tokens=100)\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "# Beispiel-Prompt\n",
    "prompt = \"User: Was sind die Vorteile von Mistral? Assistant:\"\n",
    "response = chat(prompt, model)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b49380",
   "metadata": {},
   "source": [
    "### Trainingsdaten erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "data = [\n",
    "    {\"messages\": [{\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent\"}, \n",
    "                  {\"role\": \"user\", \"content\": \"Wie geht es dir?\"}, \n",
    "                  {\"role\": \"assistant\", \"content\": \"Mir geht es gut, danke.\"}]}\n",
    "]\n",
    "\n",
    "with open('train_data.jsonl', 'w') as f:\n",
    "    for entry in data:\n",
    "        f.write(json.dumps(entry) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c4c8ef",
   "metadata": {},
   "source": [
    "### Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Definiere die Trainingsargumente\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Trainer initialisieren und trainieren\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data  # Ersetze durch dein Dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8049aef",
   "metadata": {},
   "source": [
    "### Modell speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Speichern in Google Drive\n",
    "model.save_pretrained('/content/drive/MyDrive/gguf_model')\n",
    "\n",
    "# Hochladen auf Hugging Face\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj='/content/drive/MyDrive/gguf_model',\n",
    "    path_in_repo=\"gguf_model\",\n",
    "    repo_id=\"dein_username/modellname\",\n",
    "    repo_type=\"model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0bae3",
   "metadata": {},
   "source": [
    "### Testen des trainierten Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_prompt = \"User: Was ist der Vorteil von Mistral?\"\n",
    "response = chat(test_prompt, model)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
